\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation}
\author{Christopher Oldham, Linfei Xie, Syed Ali Asar }
\date{May 2019}

\begin{document}

\maketitle

\begin{abstract}
    Generating de novo large, complex graphs that satisfy specific properties can be a difficult task, especially if the rules governing the graphs are very complex and nonlinear such as for drug-like molecular graphs. The space of possible drug-like molecular graphs is massive, and hence search algorithms are unfeasible: a generative model is necessary. Ying, Pande and Leskovec have developed such a generative algorithm in their paper \textit{Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation}. This algorithm combines reinforcement learning (RL) and generative adversarial network (GAN) paradigms to generate novel drug-like molecular graphs. This algorithm trains an actor to build molecular graphs, atom by atom to fulfill certain chemical goals as well as similarity to training examples. We set up an environment to run their algorithm, and trained the generation of novel drug-like molecules using the ZINC 250k dataset. While we did not have the computing resources for the model to fully converge, we did generate never-before-seen drug-like molecules.bluet
\end{abstract}

\section{Theoretical Background}
In some domains, novel graphs can be created by heuristic processes, iterative improvement (such as edge pruning) or even searching over the entire domain of graphs. However for some more complex domains such as molecule generation these methods will not suffice. To begin, the rules for desirable molecules are too complicated for heuristic enumeration, an some properties such as druglikeness are not very well defined from a molecular perspective. Due to the discrete nature of molecules and the ultimately quantum-mechanical rules that govern them, these rules would be incredibly complex and nonlinear even if they could be fully described.

Furthermore, the global properties of molecules are very sensitive to small changes in structure, and the search space for possible drug-like molecules may include up to $10^{60}$ candidates. It would therefore be impossible to search through all possible molecular graphs and evaluate them post hoc. It would also be impossible to accurately enumerate heuristics a priori for the generation of drug-like molecules. Therefore Ying, Pande and Leskovec at Stanford University developed the Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation (GCPN).

This approach sees molecular graph generation as a reinforcement learning problem where an actor selects what atom to bond, where, and how to a scaffold molecule. The policy for action selection is partially informed by domain specific properties and partially by an adversarial reward. The adversarial reward promotes the generation of molecules that have properties similar to molecules from a large dataset of known drug-like molecules. This reward approach draws on the very powerful generative adversarial network (GAN) paradigm which has seen large success in producing novel images. The domain specific reward comes from various chemical properties such as synthetic accessibility. Rewards are given for each atomic action, as well as for complete molecules.

While the work is largely specific to molecule generation, the algorithm is designed to be adaptable to a range of graph generation problems for which many examples exist and specific desirable properties are well defined. The authors of the paper suggest possible applications in biological and social networks.
In our work we only consider the domain of drug-like molecular graphs. There is a large dataset (ZINC 250k) for adversarial training. Furthermore it is the example used by the authors, and is arguably the most interesting domain for current research.
\subsection{Molecule Representation}
A standard way to represent and analyze molecules is called Simplified Molecular Input Line Entry System or SMILES. This uses ASCII characters to represent atoms and bonds, with substructures represented in parentheses and square brackets. For example, the SMILES representation of ethene is C=C, and the SMILES for nicotine is CN1CCC[C@H]1c2cccnc2.

While SMILES provides a standard way for tools to represent molecules for modelling and analysis, small changes in a SMILES string can represent a large change in the structure of the molecule represented thereby. For this reason GCPN instead uses molecular graphs. For a molecular graph, each vertex represents an atom and each edge is a chemical bond. Bonds of different types are represented by different (discrete) edge values. Since the molecular graph is a more direct representation of a molecule it is also more robust: small changes in the molecular graph lead only represent small changes in the molecule's structure. Since these representations are abstract graphs, however, a graphical model such as a graph convolutional network is required to interpret the molecules.
\subsection{Graph Convolutional Networks}
At a high level, a graph convolutional network is a type of neural network that takes a graph as input. Most simply, an unweighted undirected graph with $n$ vertices can be represented as two matrices: an adjacency matrix $A\in \{0,1\}^{n\times n}$ and a node feature matrix $F\in\mathbb{R}^{n\times d}$ where $d$ is the number of features for each node. The simplest GCN would define the output of each layer to be:
\[H^{i+1}=\sigma\left( AH^iW^i \right)\]
where $W^i$ is a learnable weight matrix for layer $i$ and $\sigma$ is an activation function such as ReLU. Note in this case $H^0$ would be the original feature matrix of the nodes.

Since the outputs of each layer are multiplied by the adjacency matrix $A$, the output at each layer corresponding to a node is only influenced by the values in the previous layer corresponding to adjacent nodes. In this way, each layer of a GCN can be thought of as the passing of weighted, activated `messages' between adjacent nodes. After a set number of rounds of `message passing,' the final GCN output can be passed to a standard multilayer perceptron (MLP).

Since the edges of molecular graphs can represent bonds of different non-overlapping classes (single, double, etc.), GCPN also uses
\subsection{Markov Decision Processes}
The process of building molecules is modelled as a Markov decision process (MDP). As such, the process of building each molecule satisfies the Markov property: each decision depends only on the current state and not the past history of the molecule being built. Formally, the decision process is represented as:
\[M(S, A, P, R, \gamma)\]
The states $S$ is the set of all possible partially complete or complete molecular graphs. The actions $A$ are all possible ways to bond a new atom to any $s_i\in S$. $P$ is known as the transition dynamics and represents the probability distribution of all possible actions in $A$ given any state in $S$. Discovering these transition dynamics are the goal of the GCPN algorithm. $R$ is the reward for any action in $A$ given a state in $S$. This is a combination of step-wise goal-directed rewards and an adversarial reward for fooling a discriminator network. Finally $\gamma$ is a discount factor, which reduces the value of rewards earned in later steps of the process compared to earlier rewards. The reinforcement learning actor in the molecule building environment follows this MDP: at each submolecule in $S$ it samples an action in $A$ based on $P$ (which is trained to maximise $R$). This changes the molecule, thus updating the state, until the molecule is valently complete.

\subsection{Reinforcement Learning}
In reinforcement learning (RL), the algorithm operates by means of an abstract ``actor" that decides what action to take to update the state. The actor lives in an abstract environment composed of a current state, possible actions from this state and in some cases the history of previous states. As discussed above, our actor decides on the next action bas
\subsect on the current state: the partially complete molecule. For the molecular RL environment, the state is a molecular graph and the actions are the possible ways to bond a new atom to that molecule.

The most conventional algorithm for reinforcement learning is called Q learning. In this method a function is learned that assigns a benefit value (Q) to each state-action pair. At any stage, the actor selects the action with the highest Q value to proceed. GCPN, however, uses an algorithm paradigm known as policy gradient. A policy network is a parameterized function that maps a state directly to a policy $\pi_\theta(s_i)$: a probability distribution over the potential actions. An action is then sampled from the policy according to these probabilities. The parameters $\theta$ are learned to maximize reward via a gradient ascent algorithm. Unlike Q learning, the policy is computed directly, rather than by first learning action values.

Policy gradient is better suited to more complex decision rules and large/infinite state spaces such as molecular graphs. The specific algorithm used for policy network training by GCPN is called proximal policy optimization (PPO).

\subsection{Generative Adversarial Networks}
A na\"ive approach for creating a generative neural network would be to train a classifier and then backpropagate to create inputs that maximize a particular desirable classification. It so happens that many neural networks classify noise as signal with strong confidence. These examples that `fool' a neural network are known as adversarial examples. It has been shown that the space of adversarial examples is often much larger than the space of meaningful examples, and so this na\"ive method is most likely to return nonsense.

A better approach is called a generative adversarial network (GAN). For the GAN method, two netwoks are trained together, a generator $G$ and a discriminator $D$. The discriminator is trained to distinguish outputs of $G$ from a training set of real examples. $G$, meanwhile, is trained to fool $D$: the outputs of $G$ should maximize a `real' prediction by $D$. In game theory terms this is known as a 2-player MINIMAX game, the unique convergence result of which is that $D$ outputs 0.5 for all inputs, and $G$ generates examples from the same distribution as the training dataset.

GANs have shown great success for producing novel images, and are a central idea to GCPN. Discriminators trained with the policy network generator attempt to distinguish new molecules from a training set of 250,000 known druglike molecules. Two discriminators are trained for the GCPN algorithm: for partial and complete graphs. The partial discriminator attempts to distinguish partially built molecules from subgraphs sampled from the training set of molecular graphs. The second discriminator works only on completed molecular graphs and complete training graphs. Depending on the degree to which it is `fooled,' the step-wise discriminator provides step-wise adversarial reward to the policy network. Similarly the final discriminator provides a per-molecule reward once the molecule is complete. Both of these rewards, along with other domain-specific rewards, are used to train an optimal policy for molecule building using PPO.

\subsection{Chemical Details}
\subsubsection*{Valency}
Different atoms can only make a certain number of bonds to other atoms: this property is known as valency. Valency not only informs the molecule environment of what actions are possible, but also after every step a valency check is performed to determine if the molecule is complete. If all atoms are bonded to the maximum number of other atoms, the valency check passes and that molecule is complete.

\subsubsection*{Domain-Specific Rewards}
As well as adversarial reward, there are other chemical properties that can be determined from the molecular graphs to inform goal-directed molecule building. These are computed by the RDKit package. These are QED, (which somewhat quantifies druglikeness), LogP (octanol-water partition coefficient) and molecular weight (the sum of atomic weights over the molecular graph). Other chemical rewards include synthetic accessibility, which determines how easy a molecule is to synthesize via chemical procedures, and steric strain, which measures the strain on the molecule caused by the geometry. Furthermore molecules are penalized if they contain functional groups which are known to be too reactive for the molecule to be stable.


\section{Implementation}
\subsection{OpenAI Gym}
\subsection{RDKit}
\subsection{etc.}
\section{Technical Details}
\subsection{ZINC250k Dataset}
\subsection{Operating Environment and Program Time}

\section{Results}
The generation of drug-like molecules was trained using the ZINC 250k dataset. We weren’t able to have the process converge, but we were able to keep it running for 9.9 hours, before we terminated the process. The output is a csv file that consists of each iteration the program ran. Each iteration consists of the molecule(s) generated and labeled in the SMILES representation, and several step-wise rewards. In this run, we focused on generating molecules with the highest possible penalized logP and QED scores. logP score accounts for synthetic accessibility and QED score indicates druglikeness.

When generating molecules, there are two types of rewards that guide the behavior of the RL agent: domain-specific rewards and intermediate rewards. Domain-specific rewards consist of the combination of final property scores, such as octanol-water partition coefficient (logP), druglikeness, and molecular weight. In addition, it includes the penalization of unrealistic molecules that violate steric strain and zinc functional group rules. Intermediate rewards consist of the stepwise validity rewards and adversarial rewards. For example, if no valency rules are violated then a small positive value is added to the stepwise reward. For the adversarial rewards, the GAN framework is employed to ensure the generated molecules resemble a given set of molecules. The specific rewards indicated in the output file are: \textbf{reward_valid}, \textbf{reward_qed}, \textbf{reward_sa}, \textbf{rew_env}, \textbf{rew_d_step}, \textbf{rew_d_final}, \textbf{cur_ep_ret}, \textbf{flag_steric_filter}, \textbf{flag_zinc_filter}, and \textbf{stop}.

\textbf{reward_valid} is a value of 2 if it passes the 3D conversion check, meaning that there’s no excessive strain on the molecule and it if doesn’t contain any problematic functional groups. (i.e. If flag_steric_strain_filter is TRUE and flag_zinc_molecule_filter is also TRUE, this variable receives a value of 2)
\textbf{reward_qed} is the reward for druglikeness (QED). Its values are between [0,1]. The higher the reward the better, since we are trying to maximize this value. QED stands for the quantitative estimation of drug-likeness. The QED score reflects the underlying distribution of molecular properties, such as molecular weight, logP, topological polar surface area, number of hydrogen bond donors and acceptors, number of aromatic rings and rotatable bonds, and the presence of unwanted chemical functionalities.
\textbf{reward_sa} is the reward for synthetic accessibility. Its values are normalized between [0,1]. The higher the better, because this is one of the target properties to optimize. Synthetic accessibility score is the measure of how easily the molecule synthesizes to create compounds. 
\textbf{final_stat} depends on the reward type (i.e. logP target, QED target, molecular weight target)
\textbf{rew_env} is a reward value based on an action in the certain environment, given by this formula: env.step(ac)
\textbf{rew_d_step} is mainly comprised of a step ratio and step function
\textbf{rew_d_final} is mainly comprised of the final ratio and final function
\textbf{cur_ep_ret} returns the reward in the current episode, which is the sum of the previous 3 values for all steps in the current episode.
\textbf{flag_steric_strain_filter} is TRUE if there is no excessive steric strain. This is the increase in potential energy of a molecule due to repulsion between electrons in atoms that are not directly bonded to each other. This makes it unlikely for the molecule to be stable in ordinary conditions, thus not generating a realistic molecule.
\textbf{flag_zinc_molecule_filter} is TRUE if the molecule doesn’t possess any problematic and reactive functional groups based on the zinc functional group
\textbf{stop} is TRUE if the molecule is complete and doesn’t need any more additions. However, atoms and edges can still be added to molecules in the next iteration, even when the stop value is TRUE for the current iteration.


\section{Discussion}
The last iteration that ran was able to generate 11 different molecules that fully satisfied the zinc molecule check and the steric strain check. Therefore, each molecule’s \textbf{reward_valid} variable is 2. This is not the last iteration of the process, rather the last iteration before we purposely terminated the process before convergence. That being said, these values and molecules generated are not final and it was a coincidence they all resulted in reward_valid values of 2 and stop variable was also TRUE. All other rewards, including reward_qed and reward_sa are iteration-based rewards per molecule, so these values will not necessarily converge to single values. However, we do expect the QED score and the logP scores to be optimized for the most realistic molecules produced.

\end{document}
